{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34aded9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "[TEST START] Data Length: 356513 chars\n",
      "============================================================\n",
      "\n",
      "[1] Oracle: Generating Ground Truth Query & Reference...\n",
      " -> Base Query: Please explain the following content from this document in detail: 'tains specific rules for AI systems that create a ...'\n",
      " -> Oracle Reference Length: 657 chars\n",
      "\n",
      "[2] Student: Running Orchestrator Pipeline (with Prompt Injection)...\n",
      "\n",
      "============================================================\n",
      "[RUN] Starting Pipeline Execution\n",
      "Query: Please explain the following content from this document in detail: 'tains specific rules for AI systems that create a ...'\n",
      "\n",
      "[IMPORTANT INSTRUCTION]\n",
      "After answering, you MUST provide the exact text segments from the document that you used to derive your answer.\n",
      "Wrap the cited text inside <reference> and </reference> tags.\n",
      "Example:\n",
      "Answer: ...\n",
      "Reference: <reference>The cited text from document...</reference>\n",
      "============================================================\n",
      "\n",
      "[STEP 1/8] Preprocessing data and checking cache...\n",
      "[DONE] Preprocessing completed (0.00s)\n",
      "\n",
      "[STEP 2/8] Generating embeddings and formulating adaptive strategy...\n",
      "    >>> [Cache Found] Loading previous embedding data.\n",
      "    >>> Dynamic Strategy: Scan Range(N)=1, Extraction Range(K)=1\n",
      "[DONE] Strategy formulation completed (2.37s)\n",
      "\n",
      "[STEP 3/8] Searching for candidate chunks based on vector similarity...\n",
      "    >>> Selected 11 candidates out of 135 total chunks\n",
      "[DONE] Candidate search completed (0.48s)\n",
      "\n",
      "[STEP 4/8] Performing detailed scouting analysis (Parallel processing)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scouting: 100%|██████████| 11/11 [00:00<00:00, 31.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Detailed scouting completed (0.40s)\n",
      "\n",
      "[STEP 5/8] Selecting high-value information and packing evidence...\n",
      "    >>> Included top 1 pieces of evidence for final synthesis.\n",
      "[DONE] Packaging completed (0.00s)\n",
      "\n",
      "[STEP 6/8] Synthesizing technical report (Mistral-Small)...\n",
      "[DONE] Synthesis completed (3.09s)\n",
      "\n",
      "[STEP 7/8] Refining user-friendly response...\n",
      "[DONE] Refinement completed (5.66s)\n",
      "\n",
      "[STEP 8/8] Processing finished. Logging results...\n",
      "\n",
      "============================================================\n",
      "[SUCCESS] Total processing time: 12.00s\n",
      "============================================================\n",
      "\n",
      " -> [Success] Parsed 1 citation segments.\n",
      " -> Model Answer (Preview): ### Detailed Explanation of the Proposed Regulatory Framework for AI\n",
      "\n",
      "The European Commission's prop...\n",
      " -> Cited Text Length: 1320 chars\n",
      "\n",
      "[3] Evaluation: Calculating Semantic IOU (Oracle vs. Cited Text)...\n",
      "Loading Evaluator Model (all-MiniLM-L6-v2)...\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " RESULTS REPORT (Explicit Citation Method)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " | Query Type       : Prompt Injection (<reference> enforcement)\n",
      " | Oracle Ref Length: 657\n",
      " | Cited Text Len   : 1320\n",
      " | ----------------------------------\n",
      " | Language IOU     : 0.3684  (0.0 ~ 1.0)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " => GOOD: The model cited relevant parts, but with some noise or missing info.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "import re  # Added for parsing tags\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# [1] Semantic IOU Evaluator (Metric Calculation Class)\n",
    "# ----------------------------------------------------------------\n",
    "class SemanticIOUEvaluator:\n",
    "    def __init__(self, embedding_model_name='all-MiniLM-L6-v2', threshold=0.65):\n",
    "        print(f\"Loading Evaluator Model ({embedding_model_name})...\")\n",
    "        self.encoder = SentenceTransformer(embedding_model_name)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        # Simple sentence splitting\n",
    "        if not text: return []\n",
    "        return [s.strip() for s in text.replace('\\n', '.').split('.') if len(s.strip()) > 10]\n",
    "\n",
    "    def calculate_iou(self, reference_text: str, predicted_text: str) -> float:\n",
    "        ref_sentences = self._split_into_sentences(reference_text)\n",
    "        pred_sentences = self._split_into_sentences(predicted_text)\n",
    "\n",
    "        if not ref_sentences:\n",
    "            return 0.0\n",
    "        if not pred_sentences:\n",
    "            # If model didn't cite anything, IOU is 0\n",
    "            return 0.0\n",
    "\n",
    "        ref_emb = self.encoder.encode(ref_sentences)\n",
    "        pred_emb = self.encoder.encode(pred_sentences)\n",
    "\n",
    "        # Cosine Similarity Matrix\n",
    "        sim_matrix = cosine_similarity(ref_emb, pred_emb)\n",
    "\n",
    "        # Soft Matching\n",
    "        max_sim_ref = np.max(sim_matrix, axis=1)   # Best match in Pred for each Ref sentence\n",
    "        max_sim_pred = np.max(sim_matrix, axis=0)  # Best match in Ref for each Pred sentence\n",
    "\n",
    "        intersection_ref = np.sum(max_sim_ref > self.threshold)\n",
    "        intersection_pred = np.sum(max_sim_pred > self.threshold)\n",
    "        \n",
    "        # Intersection size (average)\n",
    "        intersection = (intersection_ref + intersection_pred) / 2.0\n",
    "        \n",
    "        # Union size\n",
    "        union = len(ref_sentences) + len(pred_sentences) - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# [2] Integration Logic (Prompt Injection & Parsing)\n",
    "# ----------------------------------------------------------------\n",
    "def run_citation_iou_test(data_path: str):\n",
    "    # 0. Load Configuration\n",
    "    try:\n",
    "        from src.core.orchestrator import WorkflowOrchestrator\n",
    "    except ImportError:\n",
    "        print(\"[Error] 'src' folder not found. Please run from the project root.\")\n",
    "        return\n",
    "\n",
    "    # Test configuration\n",
    "    config = {\n",
    "        'data': {'chunk_size': 4000, 'overlap': 1000, 'batch_size': 32},\n",
    "        'logging': {'base_path': 'test_results'},\n",
    "        'experiment_name': 'citation_iou_test',\n",
    "        'strategy': {'scan_top_n': 10, 'final_top_k': 3} \n",
    "    }\n",
    "\n",
    "    # 1. Load Data\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        full_text = f.read()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[TEST START] Data Length: {len(full_text)} chars\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # 2. [Oracle Process] Simulate Ground Truth\n",
    "    print(\"\\n[1] Oracle: Generating Ground Truth Query & Reference...\")\n",
    "    \n",
    "    if len(full_text) < 1000:\n",
    "        start_idx = 0\n",
    "        end_idx = len(full_text)\n",
    "    else:\n",
    "        start_idx = random.randint(0, len(full_text) - 1000)\n",
    "        end_idx = start_idx + random.randint(300, 800)\n",
    "    \n",
    "    oracle_reference_text = full_text[start_idx:end_idx]\n",
    "    \n",
    "    # Generate Base Query\n",
    "    snippet = oracle_reference_text[:50].replace('\\n', ' ')\n",
    "    base_query = f\"Please explain the following content from this document in detail: '{snippet}...'\"\n",
    "    \n",
    "    # [CRITICAL] Prompt Injection for Explicit Citation\n",
    "    citation_instruction = (\n",
    "        \"\\n\\n[IMPORTANT INSTRUCTION]\\n\"\n",
    "        \"After answering, you MUST provide the exact text segments from the document \"\n",
    "        \"that you used to derive your answer.\\n\"\n",
    "        \"Wrap the cited text inside <reference> and </reference> tags.\\n\"\n",
    "        \"Example:\\n\"\n",
    "        \"Answer: ...\\n\"\n",
    "        \"Reference: <reference>The cited text from document...</reference>\"\n",
    "    )\n",
    "    \n",
    "    final_query = base_query + citation_instruction\n",
    "    \n",
    "    print(f\" -> Base Query: {base_query}\")\n",
    "    print(f\" -> Oracle Reference Length: {len(oracle_reference_text)} chars\")\n",
    "\n",
    "    # 3. [Student Process] Run Orchestrator Pipeline\n",
    "    print(\"\\n[2] Student: Running Orchestrator Pipeline (with Prompt Injection)...\")\n",
    "    \n",
    "    cited_text = \"\"\n",
    "    final_answer = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize Orchestrator\n",
    "        orchestrator = WorkflowOrchestrator(config)\n",
    "        \n",
    "        # Run Pipeline (No interceptor needed, we rely on model output)\n",
    "        final_answer = orchestrator.run_pipeline([data_path], final_query)\n",
    "        \n",
    "        # 4. Parse the Output (Explicit Citation Extraction)\n",
    "        # Find all content between <reference> tags\n",
    "        citations = re.findall(r\"<reference>(.*?)</reference>\", final_answer, re.DOTALL)\n",
    "        \n",
    "        if citations:\n",
    "            cited_text = \" \".join(citations).strip()\n",
    "            print(f\" -> [Success] Parsed {len(citations)} citation segments.\")\n",
    "        else:\n",
    "            print(\" -> [Warning] No <reference> tags found in the model output.\")\n",
    "            print(\"    (The model may have failed to follow instructions or hallucinated without context.)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" [Error] Pipeline Execution Failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return\n",
    "\n",
    "    print(f\" -> Model Answer (Preview): {final_answer[:100]}...\")\n",
    "    print(f\" -> Cited Text Length: {len(cited_text)} chars\")\n",
    "\n",
    "    # 5. [Evaluation] Calculate Semantic IOU\n",
    "    print(\"\\n[3] Evaluation: Calculating Semantic IOU (Oracle vs. Cited Text)...\")\n",
    "    evaluator = SemanticIOUEvaluator(threshold=0.6)\n",
    "    \n",
    "    iou_score = evaluator.calculate_iou(oracle_reference_text, cited_text)\n",
    "\n",
    "    # 6. Results Report\n",
    "    print(f\"\\n{'+'*60}\")\n",
    "    print(f\" RESULTS REPORT (Explicit Citation Method)\")\n",
    "    print(f\"{'+'*60}\")\n",
    "    print(f\" | Query Type       : Prompt Injection (<reference> enforcement)\")\n",
    "    print(f\" | Oracle Ref Length: {len(oracle_reference_text)}\")\n",
    "    print(f\" | Cited Text Len   : {len(cited_text)}\")\n",
    "    print(f\" | ----------------------------------\")\n",
    "    print(f\" | Language IOU     : {iou_score:.4f}  (0.0 ~ 1.0)\")\n",
    "    print(f\"{'+'*60}\")\n",
    "    \n",
    "    if iou_score > 0.6:\n",
    "        print(\" => EXCELLENT: The model correctly cited the evidence used.\")\n",
    "    elif iou_score > 0.3:\n",
    "        print(\" => GOOD: The model cited relevant parts, but with some noise or missing info.\")\n",
    "    elif iou_score > 0.0:\n",
    "        print(\" => POOR: The citations are barely relevant to the ground truth.\")\n",
    "    else:\n",
    "        print(\" => FAIL: No intersection found or no citations provided.\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# [4] Main Entry Point\n",
    "# ----------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dummy data for testing\n",
    "    dummy_filename = \"./data/aiact.txt\"\n",
    "    \n",
    "    os.makedirs(os.path.dirname(dummy_filename), exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(dummy_filename):\n",
    "        print(\"Creating dummy data for testing...\")\n",
    "        with open(dummy_filename, \"w\", encoding='utf-8') as f:\n",
    "            texts = []\n",
    "            full_content = \"\\n\\n\".join(texts * 20) \n",
    "            f.write(full_content)\n",
    "\n",
    "    # Execution\n",
    "    run_citation_iou_test(dummy_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057197e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Evaluator Model (all-MiniLM-L6-v2)...\n",
      "[*] Initialized Mistral Client with model: mistral-small-2506\n",
      "\n",
      "============================================================\n",
      "[TEST START] Mistral-Small Language IOU Test\n",
      "============================================================\n",
      "\n",
      "[1] Setup Scenario\n",
      " -> Context Length : 4000 chars\n",
      " -> Oracle Ref Len : 220 chars\n",
      " -> Query          : Please explain the details regarding this segment: 'cles (OJ L 60, 2.3.2013, p. 52).\\n\\n<!-- page-start-marker-25 -->\\n\\nthe  Europe...'\n",
      "\n",
      "[2] Calling Mistral API...\n",
      " -> Generation Time: 5.80s\n",
      "\n",
      "[3] Parsing Citations...\n",
      " -> Found 4 citation segments.\n",
      " -> Cited Preview: Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the ...\n",
      "\n",
      "[4] Calculating Semantic IOU...\n",
      "\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " MISTRAL-SMALL EVALUATION REPORT\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " | Model            : mistral-small-2506\n",
      " | Oracle Ref Len   : 220\n",
      " | Student Cited Len: 1849\n",
      " | ----------------------------------\n",
      " | Language IOU     : 0.1429  (Range: 0.0 - 1.0)\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      " => PARTIAL: Citations are relevant but not precise.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "try:\n",
    "    from mistralai import Mistral\n",
    "except ImportError:\n",
    "    print(\"[!] Mistral SDK not found. Install via 'pip install mistralai'\")\n",
    "    Mistral = None\n",
    "\n",
    "# ==============================================================================\n",
    "# [1] Semantic IOU Evaluator\n",
    "# ==============================================================================\n",
    "class SemanticIOUEvaluator:\n",
    "    def __init__(self, embedding_model_name='all-MiniLM-L6-v2', threshold=0.65):\n",
    "        print(f\"Loading Evaluator Model ({embedding_model_name})...\")\n",
    "        self.encoder = SentenceTransformer(embedding_model_name)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        if not text: return []\n",
    "        \n",
    "        return [s.strip() for s in text.replace('\\n', '.').split('.') if len(s.strip()) > 10]\n",
    "\n",
    "    def calculate_iou(self, reference_text: str, predicted_text: str) -> float:\n",
    "        ref_sentences = self._split_into_sentences(reference_text)\n",
    "        pred_sentences = self._split_into_sentences(predicted_text)\n",
    "\n",
    "        if not ref_sentences: return 0.0\n",
    "        if not pred_sentences: return 0.0\n",
    "\n",
    "        ref_emb = self.encoder.encode(ref_sentences)\n",
    "        pred_emb = self.encoder.encode(pred_sentences)\n",
    "\n",
    "        # Cosine Similarity Matrix\n",
    "        sim_matrix = cosine_similarity(ref_emb, pred_emb)\n",
    "\n",
    "        max_sim_ref = np.max(sim_matrix, axis=1)\n",
    "        max_sim_pred = np.max(sim_matrix, axis=0)\n",
    "\n",
    "        intersection_ref = np.sum(max_sim_ref > self.threshold)\n",
    "        intersection_pred = np.sum(max_sim_pred > self.threshold)\n",
    "        \n",
    "        intersection = (intersection_ref + intersection_pred) / 2.0\n",
    "        union = len(ref_sentences) + len(pred_sentences) - intersection\n",
    "        \n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "# ==============================================================================\n",
    "# [2] Mistral Client Wrapper\n",
    "# ==============================================================================\n",
    "class MistralGenerator:\n",
    "    def __init__(self, model_name=\"mistral-small-2506\"):\n",
    "        api_key = \"xkqaUYgXKP8lYmONgEZUGLoQTGlDAcRg\"\n",
    "        if not api_key:\n",
    "            raise ValueError(\"MISTRAL_API_KEY environment variable is not set.\")\n",
    "        \n",
    "        self.client = Mistral(api_key=api_key)\n",
    "        self.model = model_name\n",
    "        print(f\"[*] Initialized Mistral Client with model: {self.model}\")\n",
    "\n",
    "    def generate_response(self, context: str, query: str) -> str:\n",
    "        system_instruction = (\n",
    "            \"You are a precise research assistant.\\n\"\n",
    "            \"1. Answer the user's question based ONLY on the provided Context.\\n\"\n",
    "            \"2. CRITICAL: You MUST cite the exact text segments from the Context used to derive your answer.\\n\"\n",
    "            \"3. Format your citations by wrapping the exact text inside <reference> and </reference> tags.\\n\"\n",
    "            \"   Example: ...answer... Reference: <reference>exact text from context</reference>\"\n",
    "        )\n",
    "\n",
    "        user_message = (\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question: {query}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.complete(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_instruction},\n",
    "                    {\"role\": \"user\", \"content\": user_message}\n",
    "                ],\n",
    "                temperature=0.0\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"[!] API Error: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# [3] Test Logic (Mistral Specific)\n",
    "# ==============================================================================\n",
    "def run_mistral_iou_test(data_path: str):\n",
    "    evaluator = SemanticIOUEvaluator(threshold=0.7) #\n",
    "    try:\n",
    "        generator = MistralGenerator(model_name=\"mistral-small-2506\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        return\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        full_text = f.read()\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"[TEST START] Mistral-Small Language IOU Test\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    ctx_len = 4000\n",
    "    if len(full_text) > ctx_len:\n",
    "        start_ctx = random.randint(0, len(full_text) - ctx_len)\n",
    "        context_text = full_text[start_ctx : start_ctx + ctx_len]\n",
    "    else:\n",
    "        context_text = full_text\n",
    "\n",
    "    target_len = random.randint(200, 500)\n",
    "    target_start = random.randint(0, len(context_text) - target_len)\n",
    "    oracle_reference_text = context_text[target_start : target_start + target_len]\n",
    "\n",
    "    snippet = oracle_reference_text[:80].replace('\\n', ' ')\n",
    "    query = f\"Please explain the details regarding this segment: '{snippet}...'\"\n",
    "\n",
    "    print(f\"\\n[1] Setup Scenario\")\n",
    "    print(f\" -> Context Length : {len(context_text)} chars\")\n",
    "    print(f\" -> Oracle Ref Len : {len(oracle_reference_text)} chars\")\n",
    "    print(f\" -> Query          : {query}\")\n",
    "\n",
    "    print(\"\\n[2] Calling Mistral API...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    final_answer = generator.generate_response(context_text, query)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\" -> Generation Time: {elapsed:.2f}s\")\n",
    "\n",
    "    print(\"\\n[3] Parsing Citations...\")\n",
    "    citations = re.findall(r\"<reference>(.*?)</reference>\", final_answer, re.DOTALL)\n",
    "    cited_text = \" \".join(citations).strip()\n",
    "\n",
    "    if cited_text:\n",
    "        print(f\" -> Found {len(citations)} citation segments.\")\n",
    "        preview = cited_text[:100] + \"...\" if len(cited_text) > 100 else cited_text\n",
    "        print(f\" -> Cited Preview: {preview}\")\n",
    "    else:\n",
    "        print(\" -> [Warning] No <reference> tags found. (Model Failed to Follow Instructions)\")\n",
    "\n",
    "    print(\"\\n[4] Calculating Semantic IOU...\")\n",
    "    iou_score = evaluator.calculate_iou(oracle_reference_text, cited_text)\n",
    "\n",
    "    print(f\"\\n{'+'*60}\")\n",
    "    print(f\" MISTRAL-SMALL EVALUATION REPORT\")\n",
    "    print(f\"{'+'*60}\")\n",
    "    print(f\" | Model            : mistral-small-2506\")\n",
    "    print(f\" | Oracle Ref Len   : {len(oracle_reference_text)}\")\n",
    "    print(f\" | Student Cited Len: {len(cited_text)}\")\n",
    "    print(f\" | ----------------------------------\")\n",
    "    print(f\" | Language IOU     : {iou_score:.4f}  (Range: 0.0 - 1.0)\")\n",
    "    print(f\"{'+'*60}\")\n",
    "\n",
    "    if iou_score > 0.6:\n",
    "        print(\" => PASS: Mistral accurately cited the source text.\")\n",
    "    elif iou_score > 0.1:\n",
    "        print(\" => PARTIAL: Citations are relevant but not precise.\")\n",
    "    else:\n",
    "        print(\" => FAIL: Hallucination or failure to cite.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# [4] Entry Point\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    dummy_file = \"data/aiact.txt\"\n",
    "    run_mistral_iou_test(dummy_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93319504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone (3.10.19)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
